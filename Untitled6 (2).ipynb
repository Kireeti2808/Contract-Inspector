{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFKeLT6WRUch",
        "outputId": "3c76d708-9980-47be-eda9-5709a32e40b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q langchain openai pypdf faiss-cpu sentence-transformers streamlit tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "print(\"Files in /content:\", sorted(os.listdir(\"/content\"))[:50])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0cYh73QTidt",
        "outputId": "325a6e95-66bb-45f9-9022-a58bbafc6a19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in /content: ['.config', 'BAJHLIP23020V012223.pdf', 'CHOTGDP23004V012223.pdf', 'EDLHLGA23009V012223.pdf', 'HDFHLIP23024V072223.pdf', 'ICIHLIP22012V012223.pdf', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pdf_paths = [\n",
        "    \"/content/CHOTGDP23004V012223.pdf\",\n",
        "    \"/content/HDFHLIP23024V072223.pdf\",\n",
        "    \"/content/BAJHLIP23020V012223.pdf\",\n",
        "    \"/content/EDLHLGA23009V012223.pdf\",\n",
        "    \"/content/ICIHLIP22012V012223.pdf\"\n",
        "]\n",
        "\n",
        "\n",
        "import os\n",
        "for p in pdf_paths:\n",
        "    print(p, \"->\", \"FOUND\" if os.path.exists(p) else \"MISSING\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKg-0DbcTnAK",
        "outputId": "c23cc5f5-28a2-436b-d4f1-5ad8f90f7834"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CHOTGDP23004V012223.pdf -> FOUND\n",
            "/content/HDFHLIP23024V072223.pdf -> FOUND\n",
            "/content/BAJHLIP23020V012223.pdf -> FOUND\n",
            "/content/EDLHLGA23009V012223.pdf -> FOUND\n",
            "/content/ICIHLIP22012V012223.pdf -> FOUND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pypdf import PdfReader\n",
        "from langchain.schema import Document\n",
        "\n",
        "docs = []\n",
        "for p in pdf_paths:\n",
        "    reader = PdfReader(p)\n",
        "    for i, page in enumerate(reader.pages, start=1):\n",
        "        text = page.extract_text() or \"\"\n",
        "\n",
        "        text = \"\\n\".join([ln.strip() for ln in text.splitlines() if ln.strip()])\n",
        "        if not text:\n",
        "            continue\n",
        "        meta = {\"source\": os.path.basename(p), \"page\": i}\n",
        "        docs.append(Document(page_content=text, metadata=meta))\n",
        "\n",
        "print(f\"Loaded pages (documents): {len(docs)}\")\n",
        "if docs:\n",
        "    print(docs[0].metadata, docs[0].page_content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ormFVnrZTu-R",
        "outputId": "a3d9b371-5726-4f04-a2c5-88c08773f09b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pages (documents): 222\n",
            "{'source': 'CHOTGDP23004V012223.pdf', 'page': 1} CHOLAMANDALAM MS GENERAL INSURANCE COMPANY LIMITED\n",
            "Registered Office: 2nd Floor, “DARE House”,  2, N.S.C. Bose Road, Chennai – 600 001.\n",
            "Toll free: 1800 208 9100, T: +91 (0) 44 4044 5400, F: +91 (0) 44 4044 5550\n",
            "E: customercare@cholams.murugappa.com; website: www.cholainsurance.com\n",
            "IRDA Regn. No.123;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "print(\"Chunks created:\", len(chunked_docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NsxAAdbT5m7",
        "outputId": "efb5e527-e0c5-4028-a331-a20834a37cae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks created: 222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community openai pypdf faiss-cpu sentence-transformers streamlit tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCVJ1-VOUonN",
        "outputId": "869cb6e7-57fa-49fc-91ba-3b3842b6904a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.98.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.99.3-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.9.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.48.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.10)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.99.3-py3-none-any.whl (785 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.8/785.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, openai, dataclasses-json, sentence-transformers, langchain-community\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.9.0\n",
            "    Uninstalling tiktoken-0.9.0:\n",
            "      Successfully uninstalled tiktoken-0.9.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.98.0\n",
            "    Uninstalling openai-1.98.0:\n",
            "      Successfully uninstalled openai-1.98.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-1.99.3 pydantic-settings-2.10.1 python-dotenv-1.1.1 sentence-transformers-5.1.0 tiktoken-0.10.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "mBmq-4OkUtyT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3duJRdBVDiy",
        "outputId": "df7dc423-c239-476d-bd52-eff4c04a4479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.99.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n"
      ],
      "metadata": {
        "id": "proWTD8XVGQ1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "fVnpP7pXVIs0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EMBEDDING_OPTION = \"openai\"\n",
        "\n",
        "if EMBEDDING_OPTION == \"openai\":\n",
        "    from getpass import getpass\n",
        "    import os\n",
        "    if \"OPENAI_API_KEY\" not in os.environ:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key (session-only): \")\n",
        "\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    from langchain.vectorstores import FAISS\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "    vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
        "    vectorstore.save_local(\"faiss_index_openai\")\n",
        "    print(\"Saved FAISS index -> faiss_index_openai/\")\n",
        "\n",
        "elif EMBEDDING_OPTION == \"hf\":\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import numpy as np\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.embeddings.base import Embeddings\n",
        "\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    class SBEmbeddings(Embeddings):\n",
        "        def __init__(self, model):\n",
        "            self.model = model\n",
        "        def embed_documents(self, texts):\n",
        "            return [list(x) for x in self.model.encode(texts, show_progress_bar=True)]\n",
        "        def embed_query(self, text):\n",
        "            return list(self.model.encode([text])[0])\n",
        "\n",
        "    hf_embeddings = SBEmbeddings(model)\n",
        "    vectorstore = FAISS.from_documents(chunked_docs, hf_embeddings)\n",
        "    vectorstore.save_local(\"faiss_index_hf\")\n",
        "    print(\"Saved FAISS index -> faiss_index_hf/\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Set EMBEDDING_OPTION to 'openai' or 'hf'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGm28UFSVMve",
        "outputId": "7c8b2315-1ad0-4abc-89ab-f33b4c419a3d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved FAISS index -> faiss_index_openai/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!zip -r faiss_index_openai.zip faiss_index_openai || true\n",
        "!zip -r faiss_index_hf.zip faiss_index_hf || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GIVdrmwVVnn",
        "outputId": "8097a616-bef2-4998-d129-bf0eed728534"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: faiss_index_openai/ (stored 0%)\n",
            "  adding: faiss_index_openai/index.pkl (deflated 73%)\n",
            "  adding: faiss_index_openai/index.faiss (deflated 16%)\n",
            "\tzip warning: name not matched: faiss_index_hf\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r faiss_index_hf.zip . -i faiss_index_hf)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, json, re\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "\n",
        "    pass\n",
        "try:\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "except Exception:\n",
        "    from langchain.vectorstores import FAISS\n",
        "\n",
        "try:\n",
        "    from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "except Exception:\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "emb = OpenAIEmbeddings(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "vectorstore = FAISS.load_local(\"faiss_index_openai\", emb, allow_dangerous_deserialization=True)\n",
        "\n",
        "query = \"46M, knee surgery, Pune, 3-month policy\"\n",
        "fetch_k = 10\n",
        "top_k = 5\n",
        "\n",
        "try:\n",
        "    doc_score_list = vectorstore.similarity_search_with_score(query, k=fetch_k)\n",
        "except Exception as e:\n",
        "\n",
        "    doc_score_list = [(d, None) for d in vectorstore.similarity_search(query, k=fetch_k)]\n",
        "\n",
        "seen = set()\n",
        "unique = []\n",
        "for doc, score in doc_score_list:\n",
        "    key = (\" \".join(doc.page_content.split()))[:400]\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    unique.append((doc, score))\n",
        "\n",
        "print(f\"Retrieved {len(doc_score_list)} candidates, {len(unique)} unique after dedup.\")\n",
        "print(\"---- TOP RETRIEVED CHUNKS (preview) ----\")\n",
        "for i, (d, s) in enumerate(unique[:top_k], start=1):\n",
        "    print(f\"\\n[{i}] Source: {d.metadata.get('source','?')}  Page: {d.metadata.get('page','?')}  Score: {s}\")\n",
        "    print(d.page_content[:600].replace(\"\\n\", \" \") + (\"...\" if len(d.page_content) > 600 else \"\"))\n",
        "\n",
        "\n",
        "context_parts = []\n",
        "for i, (d, s) in enumerate(unique[:top_k], start=1):\n",
        "    src = d.metadata.get(\"source\", \"unknown\")\n",
        "    page = d.metadata.get(\"page\", \"unknown\")\n",
        "    snippet = d.page_content.strip()\n",
        "    snippet = snippet if len(snippet) < 2000 else snippet[:2000] + \"...\"\n",
        "    context_parts.append(f\"=== SOURCE: {src} | PAGE: {page} | SCORE: {s} ===\\n{snippet}\")\n",
        "\n",
        "context_text = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "system_msg = (\n",
        "    \"You are an AI claims assistant. You MUST return EXACTLY one valid JSON object and NOTHING ELSE. \"\n",
        "    \"Do not output markdown, code fences, backticks, commentary or explanations. \"\n",
        "    \"Return only a JSON object with keys: Decision (Approved/Denied/Unknown), Amount (number), Justification (list of clause objects). \"\n",
        "    \"Each clause object must contain: Clause (id or heading), Text (the exact clause text from CONTEXT), Source (filename), Page (page number), Relevance (why this clause matters). \"\n",
        "    \"If evidence is insufficient or conflicting, set Decision to 'Unknown' and explain inside Justification. \"\n",
        "    \"Base your answer ONLY on the evidence in CONTEXT. If multiple clauses contradict, prefer explicit clauses mentioning waiting periods or exclusions. \"\n",
        ")\n",
        "\n",
        "user_msg = (\n",
        "    f\"Query: {query}\\n\\n\"\n",
        "    \"CONTEXT (evidence from policy docs):\\n\"\n",
        "    f\"{context_text}\\n\\n\"\n",
        "    \"Return format example:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"Decision\": \"Approved\",\\n'\n",
        "    '  \"Amount\": 150000,\\n'\n",
        "    '  \"Justification\": [\\n'\n",
        "    '    {\\n'\n",
        "    '      \"Clause\": \"12.3\",\\n'\n",
        "    '      \"Text\": \"Surgical procedures including orthopedic surgeries are covered ...\",\\n'\n",
        "    '      \"Source\": \"FILE.pdf\",\\n'\n",
        "    '      \"Page\": 7,\\n'\n",
        "    '      \"Relevance\": \"Explains coverage for knee surgery.\"\\n'\n",
        "    '    }\\n'\n",
        "    '  ]\\n'\n",
        "    \"}\\n\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=700)\n",
        "prompt = system_msg + \"\\n\\n\" + user_msg\n",
        "\n",
        "\n",
        "try:\n",
        "    resp = llm.invoke(prompt)\n",
        "    raw = resp.content if hasattr(resp, \"content\") else str(resp)\n",
        "except Exception as e:\n",
        "    raw = llm(prompt) if callable(llm) else str(e)\n",
        "\n",
        "def extract_first_json(s):\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"^```[a-zA-Z]*\\n\", \"\", s)\n",
        "    s = re.sub(r\"\\n```$\", \"\", s)\n",
        "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
        "    return m.group(0) if m else None\n",
        "\n",
        "clean = extract_first_json(raw)\n",
        "if not clean:\n",
        "    print(\"\\n❌ No JSON detected from model. Raw output:\")\n",
        "    print(raw)\n",
        "else:\n",
        "    try:\n",
        "        parsed = json.loads(clean)\n",
        "        print(\"\\n✅ Parsed JSON result:\")\n",
        "        print(json.dumps(parsed, indent=2))\n",
        "    except Exception as e:\n",
        "        print(\"\\n❌ JSON parsing failed:\", e)\n",
        "        print(\"Cleaned text that we attempted to parse:\\n\", clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7UFbHK_aMce",
        "outputId": "c6dd2300-3c4e-42c3-b14d-0823503c4be8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 10 candidates, 10 unique after dedup.\n",
            "---- TOP RETRIEVED CHUNKS (preview) ----\n",
            "\n",
            "[1] Source: BAJHLIP23020V012223.pdf  Page: 28  Score: 0.377751886844635\n",
            "UIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 28 Bajaj Allianz General Insurance Co. Ltd. Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) Issuing Office: GLOBAL HEALTH CARE  Preventive treatment  Routine maternity, Routine Delivery and newborn care and Complications of childbirth  Travel costs of Insured family members in the event of an evacuation/re...\n",
            "\n",
            "[2] Source: BAJHLIP23020V012223.pdf  Page: 10  Score: 0.3823251724243164\n",
            "UIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 10 Bajaj Allianz General Insurance Co. Ltd. Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) Issuing Office: GLOBAL HEALTH CARE 49. Sum Insured or SI means the amount stated in the Policy Schedule against each relevant Section, which shall be the Company’s maximum liability under this Policy (regardless of the...\n",
            "\n",
            "[3] Source: BAJHLIP23020V012223.pdf  Page: 25  Score: 0.38430237770080566\n",
            "UIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 25 Bajaj Allianz General Insurance Co. Ltd. Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) Issuing Office: GLOBAL HEALTH CARE 15. Uterine Prolapse 16. Stones in the urinary and biliary systems 17. Surgery on ears/tonsils/ adenoids/ paranasal sinuses 18. Surgery on all internal or external tumours/ cysts/ nod...\n",
            "\n",
            "[4] Source: BAJHLIP23020V012223.pdf  Page: 29  Score: 0.3883781433105469\n",
            "UIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 29 Bajaj Allianz General Insurance Co. Ltd. Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) Issuing Office: GLOBAL HEALTH CARE D. Applicable for Part B-III (DENTAL PLAN BENEFITS FOR INTERNATIONAL COVER)  During the first year of Global Health Care Policy with Us, 30 days waiting period would be applicable fo...\n",
            "\n",
            "[5] Source: BAJHLIP23020V012223.pdf  Page: 21  Score: 0.3886139392852783\n",
            "UIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 21 Bajaj Allianz General Insurance Co. Ltd. Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) Issuing Office: GLOBAL HEALTH CARE 1. Any type gastrointestinal ulcers 2. Cataracts, 3. Any type of fistula 4. Macular Degeneration 5. Benign prostatic hypertrophy 6. Hernia of all types 7. All types of sinuses 8. Fiss...\n",
            "\n",
            "✅ Parsed JSON result:\n",
            "{\n",
            "  \"Decision\": \"Denied\",\n",
            "  \"Amount\": 0,\n",
            "  \"Justification\": [\n",
            "    {\n",
            "      \"Clause\": \"3) 30-day waiting period (Code-Excl03)\",\n",
            "      \"Text\": \"Expenses related to the treatment of any Illness within 30 days from the first Policy commencement date shall be excluded except claims arising due to an Accident, provided the same are covered.\",\n",
            "      \"Source\": \"BAJHLIP23020V012223.pdf\",\n",
            "      \"Page\": 25,\n",
            "      \"Relevance\": \"The policy has a 30-day waiting period for any illness, which includes knee surgery, and the claim is made within this period.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "st.set_page_config(page_title=\"Contract Inspector\", page_icon=\"📄\", layout=\"wide\")\n",
        "\n",
        "if \"OPENAI_API_KEY\" in st.secrets:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = st.secrets[\"OPENAI_API_KEY\"]\n",
        "else:\n",
        "    st.error(\"Please set your OpenAI API key in Streamlit Secrets.\")\n",
        "    st.stop()\n",
        "\n",
        "@st.cache_resource\n",
        "def load_vectorstore():\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    return FAISS.load_local(\"faiss_index_openai\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "vectorstore = load_vectorstore()\n",
        "\n",
        "@st.cache_resource\n",
        "def make_qa_chain():\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "qa_chain = make_qa_chain()\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ":root {\n",
        "  --bg-color-1: hsl(0, 80%, 60%);\n",
        "  --bg-color-2: hsl(120, 80%, 60%);\n",
        "  --bg-color-3: hsl(240, 80%, 60%);\n",
        "  --gradient-angle: 135deg;\n",
        "}\n",
        ".gradient-bg {\n",
        "  background: linear-gradient(var(--gradient-angle), var(--bg-color-1), var(--bg-color-2), var(--bg-color-3));\n",
        "  background-size: 200% 200%;\n",
        "  animation: gradientShift 20s ease infinite;\n",
        "}\n",
        "@keyframes gradientShift {\n",
        "  0% { background-position: 0% 50%; }\n",
        "  50% { background-position: 100% 50%; }\n",
        "  100% { background-position: 0% 50%; }\n",
        "}\n",
        ".custom-box {\n",
        "  background-color: rgba(17, 24, 39, 0.85);\n",
        "  padding: 1.5rem;\n",
        "  border-radius: 1rem;\n",
        "  box-shadow: 0 6px 24px rgba(0, 0, 0, 0.4);\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown('<div class=\"gradient-bg\" style=\"padding:2rem;min-height:100vh;\">', unsafe_allow_html=True)\n",
        "st.markdown(\"<h1 style='text-align:center;color:white;'>📄 CONTRACT INSPECTOR</h1>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<p style='text-align:center;color:white;'>Analyze policies, contracts, or emails and get clear, detailed answers instantly.</p>\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown('<div class=\"custom-box\">', unsafe_allow_html=True)\n",
        "query = st.text_area(\"Enter your query\", placeholder=\"e.g., '46M, knee surgery, Pune, 3-month policy'\")\n",
        "if st.button(\"Submit Query\"):\n",
        "    if not query.strip():\n",
        "        st.warning(\"Please enter a query.\")\n",
        "    else:\n",
        "        with st.spinner(\"Analyzing the documents...\"):\n",
        "            result = qa_chain.invoke({\"query\": query})\n",
        "            st.subheader(\"Answer\")\n",
        "            st.write(result[\"result\"])\n",
        "            with st.expander(\"Sources\"):\n",
        "                for doc in result[\"source_documents\"]:\n",
        "                    src = doc.metadata.get(\"source\", \"Unknown\")\n",
        "                    page = doc.metadata.get(\"page\", \"Unknown\")\n",
        "                    st.markdown(f\"**{src}** — page {page}\")\n",
        "                    st.write(doc.page_content[:400] + \"...\")\n",
        "st.markdown('</div>', unsafe_allow_html=True)\n",
        "st.markdown('</div>', unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpz7iKjCbBHL",
        "outputId": "ba443f9e-e062-459f-94af-3c85c7b63547"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "langchain\n",
        "langchain-community\n",
        "langchain-openai\n",
        "openai\n",
        "pypdf\n",
        "faiss-cpu\n",
        "tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hs52eR-bU9v",
        "outputId": "a48109a7-9841-4e58-97eb-86f64206a9dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .gitignore\n",
        ".streamlit/secrets.toml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkzyzCY_bV_H",
        "outputId": "94f18e46-6eeb-47ea-af71-50efcdd34bf8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting .gitignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "# 📄 Contract Inspector\n",
        "\n",
        "Contract Inspector is a Streamlit-based application that uses LangChain, FAISS, and OpenAI to let you query insurance policy documents, contracts, or other uploaded PDFs.\n",
        "It retrieves relevant clauses and presents concise answers along with source references.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Features\n",
        "- **Gradient-themed UI** with smooth animations\n",
        "- **Natural language queries** over multiple PDF documents\n",
        "- **Relevant clause extraction** with source highlighting\n",
        "- **Secure API key handling** using Streamlit Secrets\n",
        "- **FAISS vectorstore** for fast document retrieval\n",
        "\n",
        "---\n",
        "\n",
        "## 📂 Project Structure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwMwwwJdbZCH",
        "outputId": "5adfc6ef-0535-4a00-cf80-bb083c789850"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Kireeti2808\"\n",
        "!git config --global user.email \"kireeti2808@gmail.com\"\n"
      ],
      "metadata": {
        "id": "uObCtV6zdcxQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://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@github.com/Kireeti2808/contract-inspector.git\n"
      ],
      "metadata": {
        "id": "gQAZVpKmel1_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin master\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Y-x3tLex2g",
        "outputId": "d076da3d-5f39-4aed-8a42-a6f25feab547"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: src refspec master does not match any\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/Kireeti2808/contract-inspector.git'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git branch -M main\n",
        "\n",
        "\n",
        "!git push -u origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCRZo0xre7I3",
        "outputId": "698ad813-0b77-4ce7-9543-e8917389dbe8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Permission to Kireeti2808/contract-inspector.git denied to Kireeti2808.\n",
            "fatal: unable to access 'https://github.com/Kireeti2808/contract-inspector.git/': The requested URL returned error: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Kireeti2808\"\n",
        "!git config --global user.email \"kireeti2808@gmail.com\"\n"
      ],
      "metadata": {
        "id": "Khlg9bAWfO-C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "token = getpass('Enter your GitHub Personal Access Token: ')\n",
        "\n",
        "repo_url = f\"https://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@github.com/Kireeti2808/contract-inspector.git\"\n",
        "\n",
        "!git remote set-url origin $repo_url\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWVC1kzmfTfT",
        "outputId": "a5397490-f266-4693-c846-0938b8572ef3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GitHub Personal Access Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -M main\n",
        "!git push -u origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgxU6dsoffCz",
        "outputId": "b74d3865-8fc9-46e7-b154-fcc211274c49"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Permission to Kireeti2808/contract-inspector.git denied to Kireeti2808.\n",
            "fatal: unable to access 'https://github.com/Kireeti2808/contract-inspector.git/': The requested URL returned error: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from pypdf import PdfReader\n",
        "from langchain.schema import Document\n",
        "import os\n",
        "\n",
        "# Define PDF paths\n",
        "pdf_paths = [\n",
        "    \"/content/CHOTGDP23004V012223.pdf\",\n",
        "    \"/content/HDFHLIP23024V072223.pdf\",\n",
        "    \"/content/BAJHLIP23020V012223.pdf\",\n",
        "    \"/content/EDLHLGA23009V012223.pdf\",\n",
        "    \"/content/ICIHLIP22012V012223.pdf\"\n",
        "]\n",
        "\n",
        "# Load and process PDFs\n",
        "docs = []\n",
        "for p in pdf_paths:\n",
        "    reader = PdfReader(p)\n",
        "    for i, page in enumerate(reader.pages, start=1):\n",
        "        text = page.extract_text() or \"\"\n",
        "        text = \"\\n\".join([ln.strip() for ln in text.splitlines() if ln.strip()])\n",
        "        if not text:\n",
        "            continue\n",
        "        meta = {\"source\": os.path.basename(p), \"page\": i}\n",
        "        docs.append(Document(page_content=text, metadata=meta))\n",
        "\n",
        "# Split documents\n",
        "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "\n",
        "# Create and save FAISS index\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
        "vectorstore.save_local(\"faiss_index_openai\")"
      ],
      "metadata": {
        "id": "qiky_cQ3prhR"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}